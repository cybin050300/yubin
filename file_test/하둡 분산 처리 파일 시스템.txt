하둡 분산 처리 파일 시스템

하둡의 구성 : 하둡 분산 파일 시스템과 하둡 맵리듀스
하둡 분산 파일 시스템 : 사용자의 데이터들 저장하기 위해 사용.
하둡 맵리듀스 : 하둡 분산 파일 시스템에 저장된 데이터를 기반으로 데이터를 분석하는 작업을 수행.

하둡 분산 파일 시스템의 구성 : 네임노드와 데이터 노드.
하둡 맵리듀스의 구성 : 잡 트래커와 태스크 트래커.

하둡 패키기를 이용하여 
여러 서버에 하둡 분산 파일 시스템과 하둡 맵리듀스를 구축할 수 있다.

구축된 하둡 분산 파일 시스템과 하둡 맵리듀스는 
웹 인터페이스를 이용하여 상태정보를 사용자에게 제공.

명령어 인터페이스를 통해서 하둡 분산 파일 시스템과 하둡 맵리듀스에 명령을 수행시킬 수 있다.

하둡은 
방대한 데이터를 안전하게 저장하기 위한 부분과
저장된 데이터를 처리할 수 있는 플랫폼으로 구성.

하둡 분산 파일 시스템 : 
많은 양의 데이터를 안전하게 저장하기 위해 만들어 짐.
특히 분산된 환경에서 데이터를 처리하기 위해서 
많은 양의 데이터를 안전하게 저장할 수 있을 뿐만 아니라,
저장된 데이터를 빠르게 처리할 수 있도록 설계되었다.

하둡을 제대로 활용하기 위해서는
가장 기반이 되는 하둡 분산 파일 시스템에 대해 이해하고 있는 것이 중요.

1. 하둡 분산 파일 시스템의 설계

하둡 분산 파일 시스템의 탄생
2003년 구글 파일 시스템(google File System) 논문을 이용하여 설계.

구글 파일 시스템
구글은 
엄청난 양의 데이터와 이를 처리하여 값진 결과를 얻을 수 있는 스토리지 시스템의 필요에 의해 설계.

이 시스템은 
많은 데이터를 처리하기 위해 기본적으로 확장성 있게 설계.
굉장히 큰 규모로 운영할 수 있도록 설계.

큰 규모로 운영해야 하기 때문에 값싼 서버들을 이용하여 대규모로 구축할 수 있도록 설계.
많은 서버들을 스토리지 시스템으로 운영하게 되면 
디스크나 서버의 고장인 빈번하게 발생할 수 있기 때문에, 이런 고장을 대처하기 위하여
고장 방지가 가능하도록 설계.
서버와 디스크들이 굉장히 많기 때문에 
여러 명의 사용자에게 서비스할 경우 전체 성능이 높게 나오도록 설계.

하둡 분산 파일 시스템

대량의 데이터를 빠르게 처리할 수 있도록 설계.
구글 파일 시스템과 상당히 유사. 
값싼 서버들 수 백, 수 천대를 이용하여 구축 가능.

서버들의 고장 발생 시, 분산 파일 시스템은 아무런 문제없이 동작할 수 있도록 고장 방지 기능을 구현.
한 두 대의 서버로 구성하는 것보다 많은 서버를 이용하여 구축하는 것이
전체 성능이나 분산 파일 시스템의 전체 용량을 늘리는 데 유용하기 때문에
값싼 서버들을 이용하여 구축할 수 있고 높은 수준의 고장 방지 기능을 이용할 수 있다.

대량의 데이터를 처리하기 위하여
낮은 지연(latency)보다는 높은 처리량(throughput)을 지원하도록 설계되었다.

작은 파일들을 대량 처리하기 보다는 큰 파일들을 빠르게 사용자에게 전달해 줄 수 있도록 설계.
실제 작은 파일들을 여러 개 다운로드할 때의 성능과 큰 파일을 다운로드할 때의 성능을 비교해보면
큰 파일을 다운로드할 때의 평균적인 성능이 더 좋다.

메타데이터에 접근하거나, 변경하는 작업의 성능보다는 실제 사용자 데이터를 가져오는 부분이 더 좋다.

즉, 파일 시스템의 지연은 높게 설계되었지만 
처리량이 높게 나타날 수 있도록 설계되어 있고, 큰 파일들을 처리할 수 있도록 설계.

POSIX를 지원하지 않도록 설계.
POSIX를 지원하지 않는 대신, 고유의 목적에 맞도록  설계되어 
하둡 분산 파일 시스템 위에서 동작하는 응용 프로그램들의 성능을 최대화 하였다.

* POSIX (Portable Operating System Interface)
POSIX는 유닉스 시스템에서 동일한 인터페이스를 제공함으로써
이 기종의 유닉스 간 프로그램을 쉽게 이식할 수 있도록 IEEE가 제정한 인터페이스 규약.

대부분의 유닉스 파일 시스템은 POSIX를 따라서 구현되어 있다.
예) open, read, write, close와 같은 시스템 콜.

분산 환경에서 POSIX를 지원하도록 설계한다면 여러 가지 제약사항이 있기 때문에,
하둡 분산 파일 시스템은 POSIX를 지원하지 않도록 설계.
특히, 하둡에서는 테이터를 분산 환경에 
여러 개의 동일한 파일을 저장하기 때문에 파일 수정과 같은 것을 지원하지 않도록 설계되어 있다.

하둡은
사용자와 상호 작용을 통해 빠르게 동작하기 보다는 배치 프로세스에 맞게 설계.
많은 데이터를 처리하여 원하는 결과를 보여주어야 하기 때문에,
실시간으로 데이터를 처리하여 사용자에게 결과를 보여주는 것은 불가능.

하둡은 실시간 데이터 처리보다는 한번에 많은 작업을 처리할 수 있도록 배치 프로세스에 맞게 설계.

하둡 분산 파일 시스템은 WORM(Write-Once-Read-Many) 하도록 설계.
따라서 하둡 분산 파일 시스템에 있는 데이터들은 수정 불가능.
사용자가 파일을 열고 쓴 후, 닫으면 더 이상 수정이 불가능하도록 설계.
몇 바이트를 수정하려고 해도 파일을 새로 덮어써야 한다.

이런 이유는 맵리듀스와 관련.
맵리듀스를 작성하기 위한 파일 시스템은 수정 기능이 불필요.
맵리듀스를 이용하여 분석할 파일들은 수정이 필요없다.
그리고 각각의 중간 결과 파일도 독립적으로 저장되도록 되어 있으므로,
수정 기능을 추가하지 않도록 설계.

하둡 분산 파일 시스템이 여러 개의 복제본을 관리하는데,
전체 모델은 매우 간단하게 관리할 수 있음을 의미.

만약 파일의 수정이 가능하도록 설계했다면
하나의 파일 수정 시, 여러 데이터노드에 저장되는 특정 파일들을 모두 수정해주어야 한다.

특정 파일의 한 바이트만 변경되는 경우에도
동일한 파일을 저장하고 있는 모든 데이터노드를 찾아 
저장되어 있는 파일들을 한 바이트씩 모두 수정해주어야 한다.

수정이 가능했다면 
메타데이터를 관리하는 네임노드와 사용자 데이터를 관리하고 있는 데이터노드 모두에 부담.

따라서 하둡 분산 파일 시스템은 WORM하도록 설계.

* WORM(Write-Once-Read_Many)
WORM은 한 번 쓰고 많이 읽는 다는 의미.
데이터 중에서는 특별히 수정이 필요하지 않은 데이터들이 존재.
영상 파일이나 사진 파일뿐만 아니라 많은 자료들이 수정이 필요하지 않는 데이터들이 존재.

특히 하둡은 데이터 분석을 위해 존재하는 플랫폼이므로 분석을 위한 데이터들은 수정이 필요하지 않다.

그래서 하둡 분산 파일 시스템에 있는 데이터는 
WORM 특성이 있는 데이터들을 저장하도록 설계되어 데이터의 수정이 불가능,

WORM 특성을 이용함으로써 데이터 저장방식을 간단하게 만들 수 있고,
빠른 처리가 가능하도록 설계되어 있다.

하지만 WORM 특성이 있으므로 한 번 저장된 데이터는 수정되지 않고,
덮어쓰기 방식으로만 데이터를 변경할 수 있음을 명심!!!


2. 하둡 분산 파일 시스템의 전체 구조 살펴보기

하둡 분산 파일 시스템에서 클라이언트의 동작.
메타데이터 관련 명령을 위해서 네임 노드와 통신하고 
사용자 데이터를 위해서 데이터노드와 통신.

클라이언트가 메타데이터 관련 명령을 위해서 네임노드에 명령을 전달하고
사용자 데이터 읽기/쓰기를 위해서 데이터노드에 명령을 전달.

데이터 노드 간에 데이터 복제를 위해 데이터노드 간에 통신.
실제 데이터 전달을 위해서 데이터노드와 통신 한다는 것을 명심 !!!

하둡 분산 파일 시스템 구조

 




하둡 분산 파일 시스템의 네임노드와 데이터노드의 특징.

하둡 분산 파일 시스템에서 네임노드의 주된 역할
  파일 시스템의 모든 메타데이터를 저장하고 사용자에게 전달하는 것.

분산 파일 시스템이든 일반 파일 시스템이든(ext3, xfs등) 
사용자는 메타데이터 관련 명령이 가능한 빨리 수행되길 기대.

리눅스에서 가장 많이 사용되는 파일 시스템인 ext3도 간단한 구조를 가지고 있고,
사용자에게 빠르게 메타데이터를 전달하려고 노력한다.

하둡 분산 파일 시스템의 경우에는 메타데이터가 항상 네트워크를 통해서 전달되어야 한다.
일단 네트워크를 거쳐 메타데이터를 전달하게 되면 
리눅스에서 사용되는 일반적인 파일 시스템의 메타데이터 성능을 기대하기 어렵다.

그래서 하둡은 네트워크를 거치는 메타데이터를 메모리에 상주시킴으로써
사용자에게 조금 더 빠르게 메타데이터를 전달할 수 있도록 설계되어 있다.

하둡 분산 파일 시스템에서 데이터노드의 주된 역할
  사용자에게 데이터를 전달 하는 것.

사용자 파일은 블록(block) 단위로 나누어서 관리.
하둡 파일 시스템의 기본 설정은 
하나의 블록을 3개의 데이터노드에 복제하여 저장하도록 한다.

블록들을 저장해 놓기 때문에 
하나 또는 두 개의 데이터노드에 동시에 문제가 발생하더라도 
재복제 과정을 통하여 데이터가 안전하게 존재할 수 있도록 관리하고 있다.

즉, 높은 수준의 고장 방지 기능을 갖추었다고 할 수 있다.


3. 네임노드의 역할

주요 역할 : 
하둡 분산 파일 시스템의 메타데이터를 관리하는 부분과 데이터노드들을 관리하는 부분.

메타데이터는 
파일 시스템의 디렉터리 이름과 구조, 
파일 이름과 
하나의 파일을 여러 블록을 나눈 목록을 가지고 있으며, 
이 블록들이 데이터노드에 저장되어 있는 정보들을 관리.

데이터노드들의 관리는
현재 하둡 분산 파일 시스템이 사용하고 있는 데이터노드의목록과
현재 동작이 가능한지 등을 지속적으로 알아내어
데이터노드의 정보를 관리.

이 정보를 이용하여 네임노드는 
새롭게 업로드하는 블록이 어느 데이터노드에 저장되는 지, 
모든 데이터노드들이 동작하고 있어서 고장 방지를 위한 복제 수를 만족하고 있는지를 
지속적으로 알아낼 수 있다.


1. 메타데이터 관리

하둡 분산 파일 시스템이 메타데이터는
디렉터리 구조와 해당 디렉터리에 존재하는 파일들의 목록을 관리.

파일들이 블록 단위로 나뉘어졌을 때,
이 블록들의 순서를 저장하고 블록이 어느 데이터노드에 있는 지를 모두 네임노드가 저장.

네임노드 메타데이터 구조

 


1) 메타데이터의 구조
   메타데이터이 구성 요소
     디렉터리 구조
     파일
     블록과 블록들의 순서
     블록을 저장하는 데이터노드 위치

하둡 분산 파일 시스템의 디렉터리 구조 : 트리구조
트리 구조를 가지는 일반적인 파일 시스템(ext3, ntfs)과 동일한 구조.
기존 리눅스에서 사용하던 디렉터리 관련 명령어들과 동일한 명령어들을 보유.

사용자들은 
하둡에서 제공하는 명령어들을 이용하여 파일 시스템을 이용하는 것과 같이
디렉터리를 생성하거나 제거할 수 있고,
원하는 디렉터리에 복사해 놓을 수도 있다.

하둡 분산 파일 시스템의 파일에 대한 메타데이터 
파일 이름과 파일 크기, 생성 시간, 접근 권한, 파일의 소유자와 그룹에 대한 정보를 보유.
리눅스에서 사용하는 개념과 동일한 개념을 이요.

하둡 분산 파일 시스템의 파일 관련 메타데이터
parallels@localhost:~/hadoop-1.2.1$ bin/hadoop fs -ls bin
ls: Cannot access bin: No such file or directory.

하둡 분산 파일 시스템의 블록과 블록 순서
하둡 분산 파일 시스템은 하나의 파일을 그대로 저장하지 않는다.

하나의 큰 파일을 저장하려고 하면
하둡 분산 파일 시스템이 자동으로 하나의 파일을 여러 개의 블록으로 나누고,
이 블록별로 데이터노드에 저장.

블록의 크기 : 기본적으로 64MB로 설정되어 있다.

즉, 파일의 크기가 64MB를 초과할 경우
하나의 파일을 여러 개의 블록으로 나눠서 저장.

하나의 파일을 블록으로 나누고 이 블록들의 순서에 대해서 알고 있어야 하므로 
네임노드에서 파일과 관련된 블록 정보들을 보관.
하둡 분산 파일 시스템의 블록과 관련된 데이터노드의 정보
블록은 여러 개로 구성되는 데, 이 블록들을 잃어버리지 않기 위해서 
하둡 분산 파일 시스템은 동일한 블록을 여러 개 복제해 놓는다.

이 블록들이 저장되어 있는 곳이 데이터 노드.
그러므로 블록과 관련하여 
하나의 블록이 존재하는 데이터노드에 대한 정보도 네임노드가 메타데이터로 관리.

2) 메타데이터 초기화와 네임노드 포맷
기본적으로 네임노드에서 메타데이터를 사용하고 초기화하기 위해서는
네임노드를 시작하기 전에 포맷을 수행해야 한다.

포맷 동작도 일반 파일 시스템과 동일.

네임노드의 포맷을 수행하면 일반 파일 시스템과 동일하게 
루트 디렉터리를 제외하고 나머지 디렉터리에 대한 정보가 모두 없어진다.
즉, 네임노드는 메모리에 저장된 모든 메타데이터 정보를 없애고 초기화한다.

포맷된 네임노드는
루트 디렉터리부터 사용자가 디렉터리를 생성하고 업로드할 수 있다.
즉, hadoop 명령을 이용하여 
마치 일반 파일 시스템처럼 포맷하여 사용할 수 있다.

포맷 시 주의할 점.
하둡 분산 파일 시스템을 사용하던 중 포맷을 동작시키면
데이터노드에 있는 데이터노드 프로세스가 동작하지 않는다.
이유 : 
네임노드만 포맷되고 데이터노드는 포맷되지 않기 때문.
포맷을 수행할 때 하둡 분산 파일 시스템을 동작시키지 않고 수행시킨다..
네임노드에서 포맷 명령을 수행하는데, 
이 때 네임노드만 포맷되고 데이터노드가 포맷되지 않기 때문에 문제가 발생.

하둡 분산 파일 시스템처럼 구축할 수 있는 파일 시스템은
대규모로 구축할 경우 실제 데이터노드에는 중요한 정보가 많이 존재.
대규모로 구축한 경우에는 중요한 서비스를 동작시키기 때문에 포맷을 수행할 이유가 없다.

네임노드 포맷의 예
parallels@localhost:~/hadoop-1.2.1$ bin/hadoop namenode -format

13/09/04 00:22:25 INFO namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = localhost/127.0.0.1
STARTUP_MSG:   args = [-format]
STARTUP_MSG:   version = 1.2.1
STARTUP_MSG:   build = https://svn.apache.org/repos/asf/hadoop/common/branches/branch-1.2 -r 1503152; compiled by 'mattf' on Mon Jul 22 15:23:09 PDT 2013
STARTUP_MSG:   java = 1.6.0_27
************************************************************/
13/09/04 00:22:26 INFO util.GSet: Computing capacity for map BlocksMap
13/09/04 00:22:26 INFO util.GSet: VM type       = 64-bit
13/09/04 00:22:26 INFO util.GSet: 2.0% max memory = 1013645312
13/09/04 00:22:26 INFO util.GSet: capacity      = 2^21 = 2097152 entries
13/09/04 00:22:26 INFO util.GSet: recommended=2097152, actual=2097152
13/09/04 00:22:26 INFO namenode.FSNamesystem: fsOwner=parallels
13/09/04 00:22:26 INFO namenode.FSNamesystem: supergroup=supergroup
13/09/04 00:22:26 INFO namenode.FSNamesystem: isPermissionEnabled=true
13/09/04 00:22:26 INFO namenode.FSNamesystem: dfs.block.invalidate.limit=100
13/09/04 00:22:26 INFO namenode.FSNamesystem: isAccessTokenEnabled=false accessKeyUpdateInterval=0 min(s), accessTokenLifetime=0 min(s)
13/09/04 00:22:26 INFO namenode.FSEditLog: dfs.namenode.edits.toleration.length = 0
13/09/04 00:22:26 INFO namenode.NameNode: Caching file names occuring more than 10 times 
13/09/04 00:22:26 INFO common.Storage: Image file /tmp/hadoop-parallels/dfs/name/current/fsimage of size 115 bytes saved in 0 seconds.
13/09/04 00:22:26 INFO namenode.FSEditLog: closing edit log: position=4, editlog=/tmp/hadoop-parallels/dfs/name/current/edits
13/09/04 00:22:26 INFO namenode.FSEditLog: close success: truncate to 4, editlog=/tmp/hadoop-parallels/dfs/name/current/edits
13/09/04 00:22:26 INFO common.Storage: Storage directory /tmp/hadoop-parallels/dfs/name has been successfully formatted.
13/09/04 00:22:26 INFO namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at localhost/127.0.0.1
************************************************************/

디렉터리 생성의 예
parallels@localhost:~/hadoop-1.2.1$ bin/hadoop dfs -mkdir user/parallels
parallels@localhost:~/hadoop-1.2.1$ bin/hadoop dfs -ls
Found 5 items
-rw-r--r--   1 parallels supergroup     493744 2013-09-04 00:34 /user/parallels/CHANGES.txt
-rw-r--r--   1 parallels supergroup      13366 2013-09-04 00:34 /user/parallels/LICENSE.txt
-rw-r--r--   1 parallels supergroup        101 2013-09-04 00:34 /user/parallels/NOTICE.txt
-rw-r--r--   1 parallels supergroup     121130 2013-09-04 00:32 /user/parallels/build.xml
drwxr-xr-x   - parallels supergroup          0 2013-09-04 00:50 /user/parallels/user

포맷을 수행하지 않은 데이터노드의 예
 
네임노드 VERSION 파일 위치 : /tmp/hadoop-username/dfs/name/current
데이터노드 VERSION 파일 위치 : /tmp/hadoop-username/dfs/data/current

네임노드 VERSION 확인의 예
$ cat VERSION
#Sat Jul 14 04:57:32 UTC 2013
namespaceID=1098603600
cTime=0
storageType=NAME_NODE
layoutVersion=-32

데이터노드의 VERSION 확인의 예
$ cat VERSION
#Sat Jul 14 04:26:44 UTC 2013
namespaceID=869685143
storageID=DS-562507370-----50010-1342235519987
cTime=0
storageType=DATA_NODE
layoutVersion=-32


데이터노드 포맷방법
하둡에서 데이터노드 변경을 위한 두 가지 방법.

데이터노드에서 데이터를 저장하고 있는 디렉터리의 파일들을 제거.
이 디렉터리는 VERSION 파일이 존재하는 디렉터리.
여기에 있는 파일들은 rm 명령으로 제거.

VERSION 파일에 있는 namespaceID를 네임노드에 있는 값과 동일하게 변경
동일하게 변경해놓고 하둡 분산 파일 시스템을 재시작하면 
해당 디렉터리에 존재하던 데이터 파일들은 삭제되고 데이터노드가 정산적으로 재시작.

이 두 가지 방법중 하나를 이용하면 하둡 분산 파일 시스템을 초기화할 수 있다.
초기화한 후 사용할 수 있도록 준비되면 
사용자들은 자유롭게 디렉터리를 만들고 파일을 업로드/다운로드할 수 있다.


3) 메타데이터 관리
   사용자들이 디렉터리를 만들고 파일을 업로드하면 네임노드의 메타데이터가 하는 일.

디렉터리 관리.
사용자가 생성한 디렉터리는 네임노드의 메타데이터에만 존재.

네임노드는 디렉터리 관련 메타데이터를 관리하기 위해서
네임노드에 있는 FSDirectory 클래스를 이용해서 관리하고,
디렉터리 정보들을 항상 메모리에 상주시켜 놓고 있다.

디렉터리를 만들면 데이터노드에는 아무런 일도 생기지 않는다.
데이터노드에는 블록단위로 관리되는 파일들만 생성될 뿐
하둡 분산 파일 시스템의 디렉터리 구조와는 무관.

디렉터리의 관리를 위해 디렉터리별 할당량을 설정할 수 있다.
디렉터리는 사용자가 임의로 만들 수 있지만, 
디렉터리별 할당량을 지정함으로써 
특정 디렉터리에 많은 디렉터리가 생기거나 많은 파일이 생기는 것을 제한할 수 있다.

이 기능은 사용자에게는 불필요한 기능이라 생각할 수 있지만,
구축한 하둡 분산 파일 시스템을 관리하는 관리자에게는 유용하게 사용될 수 있는 기능.

관리자의 디렉터리별 파일 및 디렉터리 생성 수 변경의 예
parallels@localhost:~/hadoop-1.2.1$ bin/hadoop dfsadmin -setQuota 10 /user/parallels

관리자의 디렉터리별 파일 및 디렉터리 생성 수 삭제의 예
parallels@localhost:~/hadoop-1.2.1$ bin/hadoop dfsadmin -clrQuota /user/parallels

사용자의 디렉터리별 파일 및 디렉터리 생성 수 조회의 예
parallels@localhost:~/hadoop-1.2.1$ bin/hadoop dfs -count -q /user/parallels
          10               9            none             inf            1            0                  0 hdfs://localhost:9000/user/parallels

관리자의 디렉터리별 파일 및 디렉터리 용량 변경의 예
parallels@localhost:~/hadoop-1.2.1$ bin/hadoop dfsadmin -setSpaceQuota 10 /user/parallels
parallels@localhost:~/hadoop-1.2.1$ bin/hadoop dfs -count -q /user/parallels
        none             inf              10         -628331            3            4             628341 hdfs://localhost:9000/user/parallels

관리자의 디렉터리별 파일 및 디렉터리 용량 삭제의 예
parallels@localhost:~/hadoop-1.2.1$ bin/hadoop dfsadmin -clrSpaceQuota /user/parallels
parallels@localhost:~/hadoop-1.2.1$ bin/hadoop dfs -count -q /user/parallels
        none             inf            none             inf            3            4             628341 hdfs://localhost:9000/user/parallels





파일 관리
하둡 분산 파일 시스템에서는 파일들을 블록 단위로 나누어 관리.

하나의 큰 파일을 그대로 파일 시스템에 저장하지 않고,
지정한 블록 크기로 나누어 관리하고,
나뉜 블록들은 지정한 복제 수만큼 데이터노드에 동일한 블록을 저장.

즉 하나의 파일이 블록의 기본 크기인 64MB를 초과할 경우,
여러 개의 블록으로 나누어 데이터노드에 저장.

이 블록의 크기는 사용자가 지정한 설정에 따라 변경할 수 있다.

하둡 분산 파일 시스템 파일 및 블록의 관계

 


하나의 파일을 여러 개의 블록으로 나누어 저장하는 이유 :
하둡 분산 파일 시스템은 큰 파일(TB~PB)을 다루기 위한 파일 시스템.

파일을 그대로 데이터노드에 저장하게 된다면 
데이터노드에 설치한 파일 시스템의 최대 파일 크기 이상의 파일을 생성할 수 없다.

하지만 여러 개의 블록으로 파일을 나누어 저장하면
파일 크기와 관계없이 저장할 수 있다.

하둡 분산 파일 시스템은 
해당 파일에 대해서 블록을 어떻게 나누었는지
블록의 리스트가 어떻게 되는지
해당 블록이 어느 데이터노드에 저장되어 있었는지만 저장하고 있으면
파일 크기에 관계없이 저장할 수 있다.


하둡은 맵리듀스와 같이 큰 테이터 처리를 지원한다.
큰 데이터를 처리하기 위해서 하나의 큰 파일을 저장하면
큰 파일을 메모리로 읽어와 처리하는 데까지 많은 시간이 걸린다.

그러므로 큰 파일을 여러 개의 작은 파일로 만들어
여러 서버에 분산시켜 놓고 동시에 처리하는 것이 
훨씬 빨리 처리할 수 있다.


블록을 크게 나누었을 경우의 장점

1 클라이언트가 네임노드와 통신하는 횟수를 줄일 수 있다.
  하나의 파일을 저장하기 위해서 각 블록을 요청할 때마다 네임노드에 블록을 요청해야 하는데,
  블록 크기를 크게 함으로써 
  네임노드와 통신하는 횟수가 줄어들고,
  그만큼 네임노드의 부하를 줄일 수 있다.

2 메타데이터의 크기를 줄일 수 있다.
    각 블록마다 어느 서버에 복제본들이 있는지 알아야 한다.

    동일한 파일에 블록들이 늘어나게 되면
    메타데이터의 크기가 늘어나게 되고,
    결국 네임노드의 메모리 용량을 더 확보해야 하는 문제가 발생할 수 있다.

메타데이터를 작게 유지하기 위해서는 블록 크기가 클수록 유리.

블록을 크게 나누었을 경우의 단점
사용자 데이터를 저장하기 위하여 큰 블록을 할당하고
여기에 작은 파일만 쓰게 된다면
블록을 할당하는 과정에 부하가 발생할 수 있다.



















2. 메타데이터의 안전한 보관 - Edits와 FsImage 파일과 세컨더리 네임노드
하둡 분산 파일 시스템의 메타데이터 : 메모리에 저장
이유 : 메모리에 저장해 놓아야 빠르게 사용자에게 전달할 수 있고 관리가 용이.

하둡 분산 파일 시스템의 네임노드는 하나만 동작.
만약 모든 메타데이터를 메노리에 저장한 상태에서 네임노드가 문제가 생겨 다운로드될 경우
모든 디렉터리 구조 및 블록 관련 정보 유실.
서비스 지속 불가능.

네임노드는 
한 곳에서 고장이 발생하면 전체 시스템이 중단되는 
단일 고장점(Single point Failure)문제를 가지고 있다.

이러한 문제를 방지하기 위해 하둡 분산 파일 시스템은 
메타데이터 관련 데이터들을 파일로 저장하여
네임노드에 문제가 발생해 다운되었을 경우 빠르게 복구시킬 수 있도록 한다.
이 파일이 Edits와 FsImage.

네임노드에 문제가 발생해 꺼졌을 경우 빠르게 복구하기 위해 
세컨더리 네임노드를 이용해 제공.

1) Edits와 FsImage

Edits 파일의 주된 역할 : 
하둡 분산 파일 시스템에서 메타데이터 관련 명령 수행 목록을 기록.
하둡 분산 파일 시스템에서 명령을 수행하면 Edits 파일에 기록하게 된다.

이 방법은 기존 파일 시스템에서도 사용되었으며,
저널링(journaling), 로그(log)라는 용어를 사용.

Edits 파일의 사용 :
이 파일은 파일 시스템의 변경에 대한 로그를 기록하는 파일.
네임노드에 문제가 발생하였을 경우복구하기 위해 사용.

Edits 파일의 기록되는 경우 :
디렉터리와 파일을 추가하거나 삭제했을 때.
권한을 변경했을 때.
파일의 소유자(Owner)를 변경했을 때.
파일을 복제할 때.
관리자가 디렉터리 할당량을 변경했을 때 등.
파일 시스템의 네임스페이스(namespace)와 관련된 동작을 할 때마다 
Edits 파일에 기록.

Edits 파일은 메모리에 있는 메타데이터들을 파일로 가지고 있으므로
이 파일을 이용해서 파일 시스템의 네임스페이스를 복구할 수 있다.

Edits 파일은 파일 시스템의 로그를 관리하는 파일이지만
로그가 굉장히 많이 쌓이게 된다면 복구하는데 그만큼 많은 시간이 소요.

하둡 분산 파일 시스템에 굉장히 많은 파일을 업로드하고
관련 명령어들을 많이 수행할수록 
Edits 파일만 이용하여 네임노드를 복구하면 굉장히 많은 시간이 소요.

FsImage 의 파일 :
네임노드가 복구를 수행할 때 사용하는 파일.
Edits 파일에 기록된 내용을 병합하여 기록해 놓은 파일.

Edits 파일은 
파일 시스템 내부에서 동작하는 
모든 디렉터리와 파일 관련 명령들을 하나하나 기록해 놓기 때문에 
불필요한 내용이 많이 존재.

예) 사용자가 하나의 파일을 업로드한 후, 
나중에 필요 없게 되어 해당 파일을 삭제하였을 경우,
업로드를 수행하면서 발생한 파일 생성, 복제 생성등의 명령과
삭제과정에서 발생하는 명령을 모두 기록해놓기 때문에
현재 존재하지 않는 불필요한 내용에 대해서도 모두 저장.

그러므로 Edits에서 불필요한 내용들을 제거하고 
네임노드 복구 시 빠르게 복구할 수 있도록 FsImage 파일에 필요한 내용들만 병합해 놓는다.

Edits 파일의 관리 :
이 파일은 네임노드의 프로세스가 동작하고 있는 서버에 저장된다.
파일에 저장되므로 안전할 것이라 생각되지만 항상 그렇지는 않다.

네임노드가 문제가 발생하는 조건 :
네임노드는 프로그램의 버그에 의해 죽을 수도 있지만,
서버의 하드디스크, 메모리, CPU에 문제가 생겨 죽을 수도 있다.
네임노드의 서버에 문제가 생겨 죽었을 경우,
네임노드를 바로 복구시킬 수 없고,
최소한 서버를 복구하는 시간만큼은 운영할 수 없거나

하드디스크가 고장이라면
하둡 분산 파일 시스템의 모든 메타데이터를 잃게 된다.
  당연히 저장되어 있는 모든 데이터는 유실.

Edits 파일의 저장 위치 : 
/tmp/hadoop-parallels/dfs/name/current

edits 파일과 fsimage 파일 보기 
parallels@localhost:~/hadoop-1.2.1$ ls -l /tmp/hadoop-parallels/dfs/name/current

total 1040
-rw-rw-r-- 1 parallels parallels 1048580 Sep  6 11:41 edits
-rw-rw-r-- 1 parallels parallels    1200 Sep  6 11:35 fsimage
-rw-rw-r-- 1 parallels parallels       8 Sep  6 11:35 fstime
-rw-rw-r-- 1 parallels parallels     100 Sep  6 11:35 VERSION

여기 있는 파일들은 
하둡 분산 파일 시스템에서 자동으로 생성하고 관리하는 파일들이므로
특별한 이유가 없다면 함부로 변경하거나 삭제하면 안된다.

* tmp 디렉터리
/tmp 디렉터리는 이름 그대로 임시 파일을 저장하는 곳.

하둡을 설치하고 dfs.name.dir 같은 설정을 해주지 않으면
tmp 디렉터리에 파일들이 생기는 것을 확인할 수 있다.

이 디렉터리는 리눅스 배포판에 따라 관리하는 방법이 약간 차이가 있다.
특정 배포판에서는 재부팅할 때마다 
이 디렉터리에 있는 파일들을 모두 삭제하므로
네임노드와 데이터노드에 있는 파일들을 잃고 싶지 않다면 
해당 설정을 변경하야 다른 디렉터리에 저장.

이 파일들이 tmp 디렉터리에 있을 경우 
지워질 수 있으므로 Edits 파일과 FsImage 이 삭제되지 않도록 설정.

이 설정은 
conf/core-site.xml 파일의 hadoop.tmp.dir 설정 값을 변경하거나
conf/hdfs-site.xml 파일의 dfs.name.dir 설정 값을 변경하여 
다른 디렉터리에 저장되지 않도록 설정할 수 있다.

네임노드 저장 디렉터리 변경

 

위와 같이 설정하면
기존의 /tmp/hadoop-parallels/dfs/name/current 에 저장되던 네임노드 관련 파일들을
/home/parallels/namenode/dfs/name 에 저장되도록 변경할 수 있다.

혹시 네임노드가 재부팅되더라도 
Edits 파일과 FsImage 파일이 지워지는 것을 방지할 수 있다.

이 설정만 이용한다면 Edits 파일이 안전하데 저장되었다고 할 수 없다.

하둡 분산 파일 시스템에서는
실제 복구 시에 사용하는 Edits 파일과 FsImage 파일을 해당 서버뿐만 아니라
다른 서버에도 복제해놓음으로써 

추후 발생할 수 있는 서버 고장에 유연하게 대체할 수 있도록 한다.
이 역할을 하는 서버가 세컨더리 네임노드,

2) Secondary NameNode
세컨더리 네임노드는 이름만 보게 되면 네임노드의 역할을 대신할 수 있는 서버라고 생각되지만.
네임노드의 역할을 대신하지 않는다.

세컨더리 네임노드의 역할 :
Edits 파일을 이용해서 FsImage 파일을 병합하는 역할.

네임노드에서 Edits 파일을 FsImage 파일로 병합하기 위해서는 
네임노드를 한 번 껐다가 다시 시작하면 병합을 수행.

네임노드가 시작하면서 
Edits에 있던 로그들을 모두 병합하여 FsImage 파일로 만들어 놓는다.
이 과정을 체크포인트(CheckPoint) 라고 한다.

하지만 Edits 파일을 FsImage 파일로 병합하기 위해 
네임노드를 강제적으로 껐다가 켤 수는 없다.

따라서 Edits 파일을 주기적으로 병합하여 FsImage 파일을 생성해 줄 다른 노드가 필요.
이 역할을 수행하는 서버가 세컨더리 네임노드.

세컨더리 네임노드의 주된 역할 :
주기적으로 네임노드에 있는 Edits 파일을 가져오고
이 파일을 FsImage 파일에 병합시킨 후,
병합된 파일을 다시 네임노드로 전달하는 역할을 담당

세컨더리 네임노드를 이용한 전체구조

 

세컨더리 네임노드는 데이터노드들이나 클라이언트들과는 전혀 통신하지 않고 
오직 네임노드와 통신.

즉, 일정 조건이 되면 Edits 파일을 가져와 
FsImage 파일을 병헙하여 네임노드로 전달하는 역할만 수행.

체크포인트 역할만 수행하고 네임노드의 역할은 전혀 수행하지 않기 때문에
체크포인트 노드라고 불리기도 한다.

체크포인트 역할을 수행하는 조건 : Edits 파일을 세컨더리 네임노드로 가져오는 조건은 두가지.
Edits 파일을 마지막으로 가져온 후, 일정시간이 경과한 경우.
Edits 파일의 용량이 특정 임계치를 초과할 경우.

이 두가지 조건은 사용자가 임의로 설정할 수 있다.
기본 설정은 
Edits 파일을 가져온 후, 1시간이 초과하면 세컨더리 네임노드로 가져오거나
Edits 파일의 용량이 64MB를 초과하면 가져오도록 되어 있다.

이 설정은 conf/core-site.xml 파일에서 설정.

conf/core-site.xml 을 이용한 체크포인트 주기와 파일 크기 설정
주기는 60초, 크기는 1MB로 설정.

 

위와 같이 설정하면 
Edits 파일을 가져간 후 60초가 경과하거나,
Edits 파일이 1MB를 초과하면 

자동으로 세컨더리 네임노드로 가져와서 
Edits 파일과 FsImage 파일을 병합한 후,
다시 네임노드로 FsImage 파일을 전달해준다.

3. 데이터노드 관리
하둡 분산 파일 시스템의 네임노드는 
데이터 노드의 동작 여부 정보를 모니터링하고,
각 데이터노드에 대한 정보를 취합하는 역할도 수행

하둡 분산 파일 시스템은
클라이언트가 데이터를 가져올 때나 데이터를 업로드할 때,
클라이언트는 네임노드와 통신하고, 
네임노드는 사용자 데이터를 위해서 데이터노드와 통신.

 

이 때 클라이언트는 
사용자 데이터를 업로드할 데이터노드를 선정하기 위해서
데이터노드의 정보를 알고 있어야 
데이터를 어디로 업로드할 지 결정할 수 있다.



클라이언트는 데이터노드에 대한 정보를 관리할 수 없기 때문에 
어느 데이터노드에 업로드할 지 결정할 수 없다.

그래서 업로드할 데이터노드에 대한 정보는 
네임노드로부터 얻어오게 된다.

사용자 데이터 업로드 과정

 

클라이언트는 업로드 요청을 하기 위해
1 네임노드에게 자신이 업로드하려는 파일에 대한 정보를 전달.
2 네임노드는 자신이 관리하고 있는 데이터노드중 하나를 선정하여
3 클라인트에게 데이터노드 정보를 전달.

4 데이터노드 정보를 얻어온 클라이언트는
5 정보를 얻어온 데이터노드에게 사용자 데이터를 업로드함으로써
6 업로드 과정을 마무리.

이와 같이 네임노드는 사용자 데이터의 업로드/다운로드 처리를 하기 위해서
데이터노드에 대한 정보를 관리하고 있어야 한다


1) 관리자의 데이터노드 동작 확인
네임노드는 각 데이터노드가 현재 잘 작동되고 있는지 
동작이 멈추었는지를 알아야 한다.

이 정보를 보유하고 있어야만 
클라이언트가 접근했을 때 동작중인 데이터노드를 파악하여 
실제로 데이터를 전달할 데이터노드를 결정할 수 있다.

각 데이터노드의 동작 여부는
각 데이터노드가 네임노드에게 자신이 현재 동작중임을 알려줌으로써
네임노드가 각 데이터노드의 동작 여부를 알 수 있도록 한다.

네임노드는 관리자에게 이 정보를 보여주는 기능을 보유.
동작여부를 알 수 있는 방법
명령어를 이용하는 방법
웹 인터페이스를 이용하는 방법

명령을 이용한 데이터노드 동작 확인

parallels@localhost:~/hadoop-1.2.1$ bin/hadoop dfsadmin -report

Configured Capacity: 66583326720 (62.01 GB)
Present Capacity: 57275310080 (53.34 GB)
DFS Remaining: 57274613760 (53.34 GB)
DFS Used: 696320 (680 KB)
DFS Used%: 0%
Under replicated blocks: 0
Blocks with corrupt replicas: 0
Missing blocks: 0

-------------------------------------------------
Datanodes available: 1 (1 total, 0 dead) <= 현재 전체 데이터노드와 동작하지 않는 데이터노드 

Name: 127.0.0.1:50010  <= 첫 번째 DataNode IP
Decommission Status : Normal
Configured Capacity: 66583326720 (62.01 GB)
DFS Used: 696320 (680 KB)
Non DFS Used: 9308016640 (8.67 GB)
DFS Remaining: 57274613760(53.34 GB)
DFS Used%: 0%
DFS Remaining%: 86.02%
Last contact: Thu Sep 05 22:04:39 PDT 2013

웹 인터페이스을 이용한 데이터노드 동작 확인
http://localhost:50070

 

위의 화면에서 Live Nodes 클릭

 


2) 하트비트(Heartbeat)와 재복제(re-replication)
네임노드는 
각 데이터노드의 동작 여부를 확인 하기 위하여 하트비트를 이용하며,
데이터노드는 
네임노드에게 자신이 동작 중임을 알리기 위하여 하트비트를 전송.

하트비트 :
하둡 분산 파일 시스템과 같이 
여러 개의 서버를 하나의 분산환경으로 관리할 때 
각 노드들의 동작여부를 알아내는 고가용성을 위한 도구.

즉, 분산환경 및 여러 개의 서버를 이용하여 시스템을 구성할 경우
각 서버의 고장이 전체 시스템의 동작에 영향을 미치기 때문에
이를 주기적으로 조사하여
각 서버들의 동작 여부를 알아내도록 하는 기능.

실제로 이 기능을 이용하여 구성된 분산 환경 같은 경우에는
각 서버들의 동작 여부,
각 서버들의 자원에 대한 모니터링 수행.
동작하지 않는 서버들을 차단하는 기능까지 포함.

하둡에서도 하트비트 기능을 이용하여
서버들의 동작 여부를 알아내고
동작하지 않는 서버들을 차단하도록 하며 
각 서버들의 자원을 모니터링 하는 기능까지 포함.

하둡에서의 하트비트 이용 :
네임노드가 모든 데이터노드에 대한 정보를 보유.
데이터노드들은 자신들이 동작하고 있다는 것을 네임노드에 알리기 위해 
3초마다 한번씩 하트비트를 네임노드에 전달.

하트비트를 전달받은 네임노드는 해당 데이터노드가 동작중임을 알게된다.

하지만 문제가 발생한 데이터노드는 
하트비트 메시지를 네임노드에 전달하지 못하기 때문에 
네임노드는 하트비트 메시지를 보내지 못한 데이터노드를 동작하지 않는 것으로 판단한다.


하둡 분산 파일 시스템의 하트비트와 블록 보고(Block Reports)

 

이렇게 하트비트를 일정시간 동안 보내지 못한 데이터노드는
더 이상 클라이언트와 통신하지 못하도록 차단.

제복제 :
문제가 발생한 데이터노드는 네임노드가 데이터 전송을 차단.
네임노드는 문제가 발생한 데이터노드에 있던 블록들을 사용하지 못한다고 판단.
이 때 발생하는 것이 제복제(re-replication)과정.
 
네임노드가 특정 노드로부터 하트비트를 받지 못한다면
1 해당 데이터노드는 사용할 수 없다고 명시해둔다.
2 그럼 해당 블록들에 대한 복제 수(replication count)가 하나씩 감소.

네임노드에는 해당 데이터노드에 있던 블록들의 정보가 있기 때문에
3 해당 데이터노드에 있던 블록의 목록을 이용하여 
각 블록이 어느 데이터노드에 복제되어 있었는 지를 검색.

4 네임노드는 블록별로 추가로 복제되어 있던 데이터노드를 찾아 
해당 데이터노드에 제복제 명령을 내린다.

이렇게 함으로써 설정한 복제 수만큼을 확보하게 되고,
다시 데이터를 안전하게 보관할 수 있게 된다.

 

하둡 분산 파일 시스템의 블록 제복제 과정


3) 데이터 재배치
하둡 분산 파일 시스템은 확장성이 뛰어난 파일 시스템,

분산 파일 시스템의 용량이 모자라거나
작업을 수행하기 위해 데이터노드의 수가 부족할 수 있다.

이 때 하둡 분산 파일 시스템은 
데이터노드를 추가함으로싸 전체 파일 시스템의 용량을 사용자가 원하는 애로 늘리거나 동작중인 노드 수를 증가시킬 수 있다.

노드를 증가시키면 
새롭게 추가된 노드에는 아무런 데이터도 존재하지 않은 상태에서 시작하게 되고,
전체 파일 시스템의 용량이 확장되고,
하둡 분산 파일 시스템에서 사용할 수 있는 노드 수가 증가.

데이터노드를 추가하면 
전체 파일 시스템의 용량은 증가시킬 수 있지만, 이용률의 불균형 발생
이유 : 
새롭게 추가된 데이터노드에는 아무런 데이터가 존재하지 않기 때문에
기존에 존재하는 노드와 비교해보았을 때 데이터양의 차이가 난다.

불균형은 읽기 명령(bin/hadoop fs ?get)과 쓰기 명령(bin/hadoop fs ?put)에서 발생.

읽기 명령의 불균형 :
사용자가 파일 시스템의 읽기 명령을 수행하면
데이터는 이전에 사용하고 있던 특정 노드에만 존재하기 때문에
클라이언트는 기존에 존재하던 데이터노드에만 접속하여 데이터를 가져온다.

즉, 새롭게 추가된 노드들에게는 데이터가 없기 때문에
사용자가 읽기 명령을 수행하면 
새롭게 추가된 데이터노드는 사용되지 않는다.

쓰기 명령의 불균형 : 
하둡 분산 파일 시스템에 쓰기 명령을 이용하여
새로운 데이터를 추가하면
데이터가 적게 존재하는 곳에 데이터를 쓰도록 한다.

그러므로 쓰기 명령을 많이 수행한다면
새롭게 추가된 데이터노드에만 데이터가 추가되고
기존에 존재하던 데이터노드에는 새로운 데이터가 거의 추가되지 않는다.

그러므로 읽기/쓰기 상황 모두에서 이용률의 불균형 문제가 발생.

해결책 :
하둡 분산 파일 시스템에서는 
데이터 재배치(rebalancing)를 통해 전체 데이터노드의 이용률을 고르게 한다.

데이터 재배치 :
새롭게 추가된 데이터노드에는 처음에 사용중인 데이터가 없다.
1 사용중인 데이터노드로부터 저장중인 블록을 
2 새롭게 추가된 데이터노드에 복제하고,
3 기존 데이터노드에서 복제된 블록을 삭제하는 것.

이렇게 데이터 재배치 과정을 거치고 나면
모든 데이터노드의 데이터 저장량이 고르게 배치된다.

이 과정 후 
전체 데이터 블록이 데이터노드에 고르게 분포하기 때문에
전체적인 데이터노드의 이용율을 고르게 할 수 있는 기술.

 

하둡 분산 파일 시스템의 데이터 재배치 


데이터 재배치의 단점 : 
재배치를 수행하는 동안 데이터노드와 네트워크에 불가피하게 과부하가 발생.

과부하 발생 이유 :
재배치를 수행하는 동안 
많은 데이터들이 사용중인 데이터노드로부터 새롭게 추가된 데이터노드로 이동.
이 때 데이터노드이 디스트로부터 많은 데이터를 가져와야 하고,
네트워크를 통해 많은 데이터를 전송.

이와 같은 장단점이 있기 때문에
재배치 명령은 자동으로 수행되지 않고,
관리자에 의해 수동으로 실행되도록 설계되었다.




명령어를 이용한 재배치 수행 방법
parallels@localhost:~/hadoop-1.2.1$ bin/start-balancer.sh -threshold 0.1

starting balancer, logging to /home/parallels/hadoop-1.2.1/libexec/../logs/hadoop-parallels-balancer-localhost.out

bin/start-balancer.sh 은
많은 데이터가 존재하지 않는 상황에서 실행 시, 재배치 작업이 동작하지 않는다.
이유 : 기본적으로 존재 비율 차이가 10% 이하면 고루 배치되었다고 설정되었기 때문.
많은 데이터가 없는 상태에서도 재배치를 수행해야 하는 경우
threshold 파라미터를 이용하여 강제적으로 재배치 명령을 수행시킬 수 있다.

threshold 파라미터는
기본적으로 설정되어 있는 10% 수치를 임의로 변경할 수 있도록 해준다.

위의 예는 데이터 존재 비율의 차이를 1.0% 이하로 맞추도록 설정한 것.

재배치 수행중인 로그 확인
위의 명령을 수행하면 로그파일의 이름을 확인 할 수 있다.
(logs/hadoop-parallels-balancer-localhost.out)

parallels@localhost:~/hadoop-1.2.1$ cat logs/hadoop-parallels-balancer-localhost.out

Time Stamp               Iteration#  Bytes Already Moved  Bytes Left To Move  Bytes Being Moved
Sep 6, 2013 9:57:28 AM	0	0 KB		68.79 MB	8.17 MB
Sep 6, 2013 9:58:07 AM	1	8.17 MB		60.55 MB	8.17 MB
Sep 6, 2013 9:59:08 AM	2	66.01 MB	21.31 MB	8.17 MB
Sep 6, 2013 9:59:46 AM	3	74.2 MB		13.38 MB	8.17 MB
Sep 6, 2013 10:10:23 AM	4	82.38 MB	5.13 MB		8.17 MB

The cluster is balanced. Exiting...
Balancing took 925.0 milliseconds
            (blocks, -f) unlimited
pending signals                 (-i) 7799
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 1024
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 7799
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited

로그파일을 보면
5번 반복 작업을 통해서 재배치 작업이 완료.
총 시간은 약 925.0 milliseconds 소요.


재배치 수행 후 결과 확인
각 데이터노드별 데이터 비율 확인 가능.
parallels@localhost:~/hadoop-1.2.1$ bin/hadoop dfsadmin -report

Configured Capacity: 66583326720 (62.01 GB)
Present Capacity: 57266348032 (53.33 GB)
DFS Remaining: 57265651712 (53.33 GB)
DFS Used: 696320 (680 KB)
DFS Used%: 0%
Under replicated blocks: 0
Blocks with corrupt replicas: 0
Missing blocks: 0

-------------------------------------------------
Datanodes available: 1 (1 total, 0 dead)

Name: 127.0.0.1:50010
Decommission Status : Normal
Configured Capacity: 66583326720 (62.01 GB)
DFS Used: 696320 (680 KB)
Non DFS Used: 9316978688 (8.68 GB)
DFS Remaining: 57265651712(53.33 GB)
DFS Used%: 0%
DFS Remaining%: 86.01%
Last contact: Fri Sep 06 00:02:47 PDT 2013

재배치 시 사용하는 네트워크 대역폭 할당
재배치 명령 수행시 네트워크에 과부하가 발생하는 것을 해결하기 위한 명령.

하둡에서는 재배치를 수행할 때 
네트워크의 대역폭을 할당할 수 있도록 제공,
parallels@localhost:~/hadoop-1.2.1$ bin/hadoop dfsadmin -setBalancerBandwidth 1048576

재배치 동작 대역폭 설정 : setBalancerBandwidth 파라미터
전송 바이트 수 : 파라미터 뒤에 명시


4. 데이터노드의 역할

데이터노드의 역할 : 사용자 데이터를 저장하는 것.
네임노드의 역할 : 데이터를 관리

1 . 블록 관리

데이터노드는 하나의 파일을 여러 개의 블록으로 나누어 관리.

블록 : 
모든 운영체제의 파일 시스템에서 사용하는 개념.

모든 운영체제에서 사용하는 파일 시스템에서는 
하나의 파일을 저장하기 위해 블록 단위로 파일을 쪼개어 디스크에 저장.

리눅스에서 많이 사용되는 ex4 파일 시스템의 경우
4KB 단위로 블록을 구성하고 파일을 쪼개서 관리.

ex4와 하둡 분산 파일 시스템 블록의 차이 :
ex4 의 경우 : 
하나의 블록이 파일에 할당되면 블록을 다 쓰지 않더라도 남은 공간 활용 불가.

하둡 분산 파일 시스템의 경우 : 
블록이 할당하더라도 블록 크기만큼 쓰지 않으면 다른 파일에 의해 사용 가능.

하둡 분산 파일 시스템은 
큰 파일을 여러 개 블록으로 쪼개어 관리하기 위한 용도로 사용되므로 
블록 크기만큼 무조건 공간을 할당하지 않는다.
즉, 하둡 분산 파일 시스템은 모든 파일을 블록 크기로 나누어 관리.

데이터노드에 저장된 블록 목록
parallels@localhost:~/hadoop-1.2.1$ cd /tmp/hadoop-parallels/dfs/data/current

parallels@localhost:/tmp/hadoop-parallels/dfs/data/current$ ls -lh

total 656K
-rw-rw-r-- 1 parallels parallels 119K Sep  6 11:32 blk_4000337845963925282
-rw-rw-r-- 1 parallels parallels  955 Sep  6 11:32 blk_4000337845963925282_1002.meta
-rw-rw-r-- 1 parallels parallels    4 Sep  6 11:30 blk_-4665610983880132182
-rw-rw-r-- 1 parallels parallels   11 Sep  6 11:30 blk_-4665610983880132182_1001.meta
-rw-rw-r-- 1 parallels parallels 483K Sep  6 11:32 blk_6306728618031891302
-rw-rw-r-- 1 parallels parallels 3.8K Sep  6 11:32 blk_6306728618031891302_1003.meta
-rw-rw-r-- 1 parallels parallels  101 Sep  6 11:32 blk_-707878366868947945
-rw-rw-r-- 1 parallels parallels   11 Sep  6 11:32 blk_-707878366868947945_1005.meta
-rw-rw-r-- 1 parallels parallels  14K Sep  6 11:32 blk_9154475286919746069
-rw-rw-r-- 1 parallels parallels  115 Sep  6 11:32 blk_9154475286919746069_1004.meta
-rw-rw-r-- 1 parallels parallels  481 Sep  6 12:10 dncp_block_verification.log.curr
-rw-rw-r-- 1 parallels parallels  153 Sep  6 11:30 VERSION

* 하둡 분산 파일 시스템에 
64MB 이상인 파일들과 64MB 이하인 파일들을 함께 업로드 후 테스트 요망
테스트 결과 : 64MB로 나뉜 블록들이 존재하고 64MB 이하인 블록들이 함께 존재.

데이터노드에서 
모든 블록이 블록 크기만큼 공간이 할당되지 않고, 
필요한 공간만큼을 차지하는 것을 확인할 수 있다.

블록 관련 옵션 변경 방법
하둡 분산 파일 시스템 기본 블록 크기는 64MB.

1) 블록의 크기 변경 방법 :
하둡 분산 파일 시스템이 동작 중에는 설정할 수 없다.

블록 크기 설정은 
하둡 분산 파일 시스템이 동작하기 전에 완료.
동작 중인 하둡 분산 파일 시스템을 중지 시키고 재설정.

블록 크기를 변경하기 위해서는 conf/hdfs-site.xml 파일을 수정.

 


블록의 크기를 변경해 블록의 크기가 더 커진다면
데이터 읽기 성능이 더 좋아진다.
하나의 파일을 덜 쪼개도 되고,
데이터노드로부터 클라이언트에게 파일을 전달하는 작업이 더 작은 오버헤드가 발생
맵리듀스 작업 환경에서도 이점

하둡 공식 홈페이지의 클러스터 가이드에서도 128MB 블록 크기를 이용
아마존 웹 서비스에서 제공하는 Elastic Map Reduce 환경에서도 128 MB 블록 크기를 이용
굉장히 큰 파일들을 많이 다루는 환경에서는 128MB 블록 크기를 많이 이용.

2) 블록들이 저장되는 위치
데이터노드에서 블록은 지정한 디렉터리에 저장된다.
지정한 디렉터리는 하둡 분산 파일 시스템의 설정에 따라 변경될 수 있다.



데이터노드가 저장할 디렉터리는 conf/hdfs-site.xml 파일을 이용하여 설정가능.
데이터노드가 데이터를 저정하는 디렉터리의 기본 설정은 dfs.data.dir 설정에 명시.
이 값은 ${hadoop.tmp.dir}/dfs/data. 이다.
${hadoop.tmp.dir} 부분은
 conf/core-site.xml 파일에 설정할 수 있는 hadoop.tmp.dir 설정값을 참조한다는 의미

hadoop.tmp.dir의 기본 값이 /tmp/hadoop-${user.name}으로 설정되어 있다.
${user.name}은
현재 네임노드와 데이터노드를 실행하는 리눅스 계정이름을 의미.

conf/core-site.xml 파일과 conf/hdfs-site.xml 파일을 변경하지 않는다면
/tmp/hadoop-사용자계정/dfs/data 로 데이터노드의 저장 디렉터리가 지정된다.

기본 설정을 이용한 디렉터리의 문제
이 디렉터리는 임시 디렉터리.
특정상황이 되면 임시 디렉터리에 있는 모든 데이터들이 제거될 수 있다.
데이터를 잃지 않기 위해 다른 디렉터리에 저장.

설정을 적용하기 위해서 
데이터노드를 구동하기 전에 설정을 명시하고 
데이터노드을 동작시켜야 한다.

데이터노드 저장디렉터리 변경
시용자 계정 : parallels  디렉터리 : hadoop
의 값으로 변경하게 되면 

데이터노드가 저장하는 디렉터리가 
/home/parallels/hadoop/dfs/data 로 변경된다.

이 디렉터리는 사용자 디렉터리이므로 
임시 디렉터리와는 달리 사용자의 데이터가 제거되지 않는다.

구동시켜보면 /home/parallels 디렉터리 밑에 
hadoop 디렉터리가 자동으로 생성되고,

하위 디렉터리 밑에 
데이터 노드에 관련된 파일들이 생성되는 것을 확인할 수 있다.
 


* hadoop.tmp.dir 의 변경
hadoop.tmp.dir 을 변경하면
데이터노드가 데이터를 저장하는 디렉터리뿐만 아니라,
다른 디렉터리도 변경할 수 있다.

hadoop.tmp.dir 은 다른 속성에 참조되는 값이다.

이 속성을 데이터노드가 동작하는 서버가 아닌 
네임노드가 동작하는 서버에서 변경하면
네임노드의 메타데이터(FsImage)와 
로그파일(Edits)을 
저장하는 디렉터리도 변경된다.

그리고 맵리듀스 관련된 노드가 동작하는 서버에서 설정하면
맵리듀스 관련된 파일을 저장하는 디렉터리도 변경된다.




2. 데이터 복제와 과정
하둡에서는 데이터를 여러 개의 데이터 노드에 복제해놓는다.
다른 분산 파일 시스템에서도 복제를 이용하여 동일한 데이터를 여러 곳에 복제해놓는데,
그 이유는 
서버의 고장과 하드 디스크 고장으로 인한 손실을 막기 위해서이다.

실제로 대규모로 구축된 분산 파일 시스템의 경우
디스크 고장으로부터 데이터를 보존하기 위해서
다른 서버에 동일한 데이터를 여러 개 복제해놓는다.

그리고 고장이 발생하면 복제되어 있던 
다른 서버에 동일한 데이터가 존재하므로 
사용자가 문제없이 데이터를 가져올 수 있다.

하둡도 데이터 복제를 이용하여 데이터 손실을 방지.

하둡 분산 파일 시스템은
사용자 데이터를 업로드와 동시에 여러 데이터 노드에 존재하도록 복제를 수행.

사용자가 데이터를 업로드하면 데이터노드에 데이터가 저장되는데,
이 때 하나의 데이터노드에만 업로드한 후,
나중에 복제한다면 데이터를 잃을 수 있는 확률이 생김.

예) 사용자가 데이터를 업로드해서 하나의 데이터노드에만 데이터가 존재한다고 가정.
이 때 데이터노드는 
업로드된 사용자 데이터를 다른 데이터노드에 복제를 수행.

복제를 수행하는 도중에 
데이터노드 혹은 디스크가 고장이 발생한다면 해당 데이터를 잃게 된다.

하둡 분산 파일 시스템은 
사용자가 데이터를 업로드하면 동시에 여러 개의 데이터노드에 복제해놓는다.

하둡은 데이터 복제를 위해 복제 파이프라이닝 방법을 이용.

복제 파이프라이닝(pipelining)
클라이언트가 데이터를 업로드하기 위해서
1 파일 업로드 요청
2 업로드할 데이터노드 정보를 네임노드로부터 전달받는다
이 때 네임노드는 업로드할 데이터노드들에 대한 정보를 알려준다.
데이터 정보를 얻어온 클라이언트는

3 사용자 데이터 일부 업로드를 3개의 데이터노드 중 첫번째 데이터노드에
데이터를 업로드하기 시작.

이 때 첫 번째 데이터노드는 4KB 크기의 일부 데이터를 클라이언트로부터 전달받는다.

4 클라이언트의일부 데이터를 받은 첫 번째 데이터노드는
데이터를 자신의 디스크에 써넣고,
써넣은 데이터를 두 번째 데이터노드에 전달.

5 두 번째 데이터노드 역시 같은 방법으로 세 번째 데이터노드에게 전달.
6 세 번째 데이터노드는 전달받은 데이터를 자신의 디스크에 써 넣음으로써
7 클라이언트 데이터 전송과 함께 복제 파이프라이닝을 수행.

클라이언트와 데이터노드들은 3,4,5,6 과정을 
모든 데이터가 전송될 때까지 수행하여 전송과 동시에 데이터를 복제할 수 있도록 한다.

이와 같이 복제파이프라이닝을 하면 데이터 손실을 방지할 수 있다.
 
복제 파이프라이닝 방법

 





하둡 분산 파일 시스템에서는 복제를 위한 설정이 존재.
하둡 분산 파일 시스테의 기본 설정은 동일한 블록을 3개 복제.
복제 개수는 사용자에 의해서 임의로 변경 가능.

변경방법은 conf/hdfs-site.xml 파일에서 설정.
이 설정은 네임노드가 동작하는 서버에서 설정.
dfs.replication 설정에 따라 사용자가 업로드한 데이터의 복제 수가를 결정.

설정을 변경했으면 네임노드를 구동시키면 복제 수가 설정된다.
이 설정은 동작 중에도 변경 가능.
동작중 설정을 변경했을 경우, 네임노드를 껐다가 켜면 시스템 설정이 변경된다.

설정을 변경하기 전에 임의의 데이터를 업로드 한 후, 복제 수를 확인.

복제 수 확인방법
stat 명령을 이용하여 파일의 메타데이터를 확인할 수 있다.
stat 명령과 뒤의 %r을 이용하면 해당 파일의 복제 수를 확인할 수 있다.

parallels@localhost:/tmp/hadoop-parallels/dfs$ cd ~/

parallels@localhost:~$ cd hadoop-1.2.1

parallels@localhost:~/hadoop-1.2.1$ bin/hadoop fs -stat %r 3Replica.txt
3

복제 수는 항상 설정된 데로만 적용되는 것은 아니며
복제 수를 변경하기 위한 다른 방법도 존재.












